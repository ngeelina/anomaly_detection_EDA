{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_card_df = pd.read_csv('creditcard.csv') # read data from the csv file\n",
    "credit_card_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize and Scale data\n",
    "#### Scale the data to similar magnitude "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "credit_card_df_norm = credit_card_df\n",
    "credit_card_df_norm['Time'] = StandardScaler().fit_transform(credit_card_df_norm['Time'].values.reshape(-1, 1))\n",
    "credit_card_df_norm['Amount'] = StandardScaler().fit_transform(credit_card_df_norm['Amount'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test split\n",
    "#### The dataset is splitted to 80% traning data and 20% testing data. Random seed is used in order to produce the same data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "RANDOM_SEED = 42 \n",
    "TEST_PCT = 0.2\n",
    "\n",
    "X_train, X_test = train_test_split(credit_card_df_norm, test_size=TEST_PCT, random_state=RANDOM_SEED)\n",
    "X_train = X_train[X_train.Class == 0] # normal transactions\n",
    "X_train = X_train.drop(['Class'], axis=1) #drop the class column\n",
    "\n",
    "\n",
    "Y_test = X_test['Class'] #save the class column for the test set\n",
    "X_test = X_test.drop(['Class'], axis=1) #drop the class column\n",
    "\n",
    "X_train = X_train.values #transform to ndarray\n",
    "X_test = X_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import regularizers\n",
    "\n",
    "nb_epoch = 50\n",
    "batch_size = 64\n",
    "input_dim = X_train.shape[1] \n",
    "encoding_dim = 14\n",
    "hidden_dim = int(encoding_dim / 2)\n",
    "learning_rate = 1e-7\n",
    "\n",
    "input_layer = Input(shape=(input_dim, ))\n",
    "encoder = Dense(encoding_dim, activation=\"tanh\", activity_regularizer=regularizers.l1(learning_rate))(input_layer)\n",
    "encoder = Dense(hidden_dim, activation=\"relu\")(encoder)\n",
    "decoder = Dense(hidden_dim, activation='tanh')(encoder)\n",
    "decoder = Dense(input_dim, activation='relu')(decoder)\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 227451 samples, validate on 56962 samples\n",
      "Epoch 1/50\n",
      "227451/227451 [==============================] - 20s 88us/step - loss: 0.8363 - acc: 0.5271 - val_loss: 0.8062 - val_acc: 0.6283\n",
      "Epoch 2/50\n",
      "227451/227451 [==============================] - 19s 82us/step - loss: 0.7681 - acc: 0.6345 - val_loss: 0.7873 - val_acc: 0.6514\n",
      "Epoch 3/50\n",
      "227451/227451 [==============================] - 18s 78us/step - loss: 0.7463 - acc: 0.6576 - val_loss: 0.7678 - val_acc: 0.6638\n",
      "Epoch 4/50\n",
      "227451/227451 [==============================] - 17s 76us/step - loss: 0.7354 - acc: 0.6701 - val_loss: 0.7608 - val_acc: 0.6816\n",
      "Epoch 5/50\n",
      "227451/227451 [==============================] - 17s 76us/step - loss: 0.7294 - acc: 0.6844 - val_loss: 0.7562 - val_acc: 0.6910\n",
      "Epoch 6/50\n",
      "227451/227451 [==============================] - 18s 77us/step - loss: 0.7252 - acc: 0.6958 - val_loss: 0.7531 - val_acc: 0.7034\n",
      "Epoch 7/50\n",
      "227451/227451 [==============================] - 17s 74us/step - loss: 0.7219 - acc: 0.7046 - val_loss: 0.7503 - val_acc: 0.7126\n",
      "Epoch 8/50\n",
      "227451/227451 [==============================] - 18s 78us/step - loss: 0.7199 - acc: 0.7101 - val_loss: 0.7483 - val_acc: 0.7167\n",
      "Epoch 9/50\n",
      "227451/227451 [==============================] - 18s 80us/step - loss: 0.7182 - acc: 0.7129 - val_loss: 0.7477 - val_acc: 0.7114\n",
      "Epoch 10/50\n",
      "227451/227451 [==============================] - 17s 77us/step - loss: 0.7169 - acc: 0.7141 - val_loss: 0.7463 - val_acc: 0.7182\n",
      "Epoch 11/50\n",
      "227451/227451 [==============================] - 18s 78us/step - loss: 0.7160 - acc: 0.7151 - val_loss: 0.7457 - val_acc: 0.7210\n",
      "Epoch 12/50\n",
      "227451/227451 [==============================] - 17s 75us/step - loss: 0.7149 - acc: 0.7172 - val_loss: 0.7478 - val_acc: 0.7124\n",
      "Epoch 13/50\n",
      "227451/227451 [==============================] - 17s 75us/step - loss: 0.7140 - acc: 0.7191 - val_loss: 0.7462 - val_acc: 0.7232\n",
      "Epoch 14/50\n",
      "227451/227451 [==============================] - 17s 74us/step - loss: 0.7135 - acc: 0.7198 - val_loss: 0.7439 - val_acc: 0.7224\n",
      "Epoch 15/50\n",
      "227451/227451 [==============================] - 16s 72us/step - loss: 0.7134 - acc: 0.7197 - val_loss: 0.7484 - val_acc: 0.7153\n",
      "Epoch 16/50\n",
      "227451/227451 [==============================] - 16s 71us/step - loss: 0.7129 - acc: 0.7201 - val_loss: 0.7454 - val_acc: 0.7221\n",
      "Epoch 17/50\n",
      "227451/227451 [==============================] - 16s 71us/step - loss: 0.7127 - acc: 0.7213 - val_loss: 0.7433 - val_acc: 0.7222\n",
      "Epoch 18/50\n",
      "227451/227451 [==============================] - 16s 70us/step - loss: 0.7128 - acc: 0.7210 - val_loss: 0.7447 - val_acc: 0.7192\n",
      "Epoch 19/50\n",
      "227451/227451 [==============================] - 16s 72us/step - loss: 0.7121 - acc: 0.7211 - val_loss: 0.7429 - val_acc: 0.7217\n",
      "Epoch 20/50\n",
      "227451/227451 [==============================] - 17s 74us/step - loss: 0.7122 - acc: 0.7213 - val_loss: 0.7437 - val_acc: 0.7219\n",
      "Epoch 21/50\n",
      "227451/227451 [==============================] - 16s 73us/step - loss: 0.7121 - acc: 0.7215 - val_loss: 0.7435 - val_acc: 0.7229\n",
      "Epoch 22/50\n",
      "227451/227451 [==============================] - 16s 72us/step - loss: 0.7119 - acc: 0.7222 - val_loss: 0.7430 - val_acc: 0.7244\n",
      "Epoch 23/50\n",
      "227451/227451 [==============================] - 16s 71us/step - loss: 0.7116 - acc: 0.7220 - val_loss: 0.7442 - val_acc: 0.7233\n",
      "Epoch 24/50\n",
      "227451/227451 [==============================] - 16s 69us/step - loss: 0.7116 - acc: 0.7208 - val_loss: 0.7441 - val_acc: 0.7269\n",
      "Epoch 25/50\n",
      "227451/227451 [==============================] - 16s 70us/step - loss: 0.7118 - acc: 0.7223 - val_loss: 0.7443 - val_acc: 0.7224\n",
      "Epoch 26/50\n",
      "227451/227451 [==============================] - 16s 72us/step - loss: 0.7115 - acc: 0.7218 - val_loss: 0.7426 - val_acc: 0.7299\n",
      "Epoch 27/50\n",
      "227451/227451 [==============================] - 17s 74us/step - loss: 0.7114 - acc: 0.7215 - val_loss: 0.7427 - val_acc: 0.7222\n",
      "Epoch 28/50\n",
      "227451/227451 [==============================] - 15s 64us/step - loss: 0.7113 - acc: 0.7205 - val_loss: 0.7423 - val_acc: 0.7218\n",
      "Epoch 29/50\n",
      "227451/227451 [==============================] - 17s 73us/step - loss: 0.7107 - acc: 0.7207 - val_loss: 0.7437 - val_acc: 0.7207\n",
      "Epoch 30/50\n",
      "227451/227451 [==============================] - 16s 71us/step - loss: 0.7107 - acc: 0.7203 - val_loss: 0.7421 - val_acc: 0.7233\n",
      "Epoch 31/50\n",
      "227451/227451 [==============================] - 15s 67us/step - loss: 0.7107 - acc: 0.7203 - val_loss: 0.7431 - val_acc: 0.7220\n",
      "Epoch 32/50\n",
      "227451/227451 [==============================] - 15s 66us/step - loss: 0.7104 - acc: 0.7199 - val_loss: 0.7436 - val_acc: 0.7172\n",
      "Epoch 33/50\n",
      "227451/227451 [==============================] - 16s 71us/step - loss: 0.7102 - acc: 0.7208 - val_loss: 0.7425 - val_acc: 0.7181\n",
      "Epoch 34/50\n",
      "227451/227451 [==============================] - 17s 73us/step - loss: 0.7101 - acc: 0.7201 - val_loss: 0.7437 - val_acc: 0.7214\n",
      "Epoch 35/50\n",
      "227451/227451 [==============================] - 17s 73us/step - loss: 0.7100 - acc: 0.7212 - val_loss: 0.7420 - val_acc: 0.7256\n",
      "Epoch 36/50\n",
      "227451/227451 [==============================] - 16s 71us/step - loss: 0.7101 - acc: 0.7199 - val_loss: 0.7421 - val_acc: 0.7218\n",
      "Epoch 37/50\n",
      "227451/227451 [==============================] - 16s 71us/step - loss: 0.7101 - acc: 0.7200 - val_loss: 0.7416 - val_acc: 0.7246\n",
      "Epoch 38/50\n",
      "227451/227451 [==============================] - 17s 75us/step - loss: 0.7094 - acc: 0.7210 - val_loss: 0.7442 - val_acc: 0.7214\n",
      "Epoch 39/50\n",
      "227451/227451 [==============================] - 16s 71us/step - loss: 0.7096 - acc: 0.7214 - val_loss: 0.7413 - val_acc: 0.7253\n",
      "Epoch 40/50\n",
      "227451/227451 [==============================] - 16s 72us/step - loss: 0.7098 - acc: 0.7209 - val_loss: 0.7412 - val_acc: 0.7235\n",
      "Epoch 41/50\n",
      "227451/227451 [==============================] - 17s 74us/step - loss: 0.7093 - acc: 0.7206 - val_loss: 0.7427 - val_acc: 0.7174\n",
      "Epoch 42/50\n",
      "227451/227451 [==============================] - 17s 73us/step - loss: 0.7092 - acc: 0.7202 - val_loss: 0.7420 - val_acc: 0.7213\n",
      "Epoch 43/50\n",
      "227451/227451 [==============================] - 16s 72us/step - loss: 0.7092 - acc: 0.7207 - val_loss: 0.7422 - val_acc: 0.7239\n",
      "Epoch 44/50\n",
      "227451/227451 [==============================] - 16s 72us/step - loss: 0.7091 - acc: 0.7208 - val_loss: 0.7431 - val_acc: 0.7178\n",
      "Epoch 45/50\n",
      "227451/227451 [==============================] - 17s 74us/step - loss: 0.7091 - acc: 0.7200 - val_loss: 0.7417 - val_acc: 0.7234\n",
      "Epoch 46/50\n",
      "227451/227451 [==============================] - 16s 71us/step - loss: 0.7087 - acc: 0.7203 - val_loss: 0.7408 - val_acc: 0.7230\n",
      "Epoch 47/50\n",
      "227451/227451 [==============================] - 16s 70us/step - loss: 0.7092 - acc: 0.7202 - val_loss: 0.7410 - val_acc: 0.7213\n",
      "Epoch 48/50\n",
      "227451/227451 [==============================] - 15s 68us/step - loss: 0.7087 - acc: 0.7206 - val_loss: 0.7425 - val_acc: 0.7242\n",
      "Epoch 49/50\n",
      "227451/227451 [==============================] - 16s 70us/step - loss: 0.7087 - acc: 0.7203 - val_loss: 0.7420 - val_acc: 0.7212\n",
      "Epoch 50/50\n",
      "227451/227451 [==============================] - 16s 71us/step - loss: 0.7087 - acc: 0.7198 - val_loss: 0.7408 - val_acc: 0.7223\n"
     ]
    }
   ],
   "source": [
    "autoencoder.compile(metrics=['accuracy'], loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "cp = ModelCheckpoint(filepath=\"fraud_detection_autoencoder.h5\", save_best_only=True, verbose=0)\n",
    "\n",
    "history = autoencoder.fit(X_train, X_train, epochs=nb_epoch, batch_size=batch_size, shuffle=True, \n",
    "                          validation_data=(X_test, X_test), verbose=1, callbacks=[cp]).history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation and Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history['loss'], linewidth=2, label='Train')\n",
    "plt.plot(history['val_loss'], linewidth=2, label='Test')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_predictions = autoencoder.predict(X_test)\n",
    "mse = np.mean(np.power(X_test - X_test_predictions, 2), axis=1)\n",
    "error_df = pd.DataFrame({'Reconstruction_error': mse,\n",
    "                        'True_class': Y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precision_rt, recall_rt, threshold_rt = precision_recall_curve(error_df.True_class, error_df.Reconstruction_error)\n",
    "plt.plot(recall_rt, precision_rt, linewidth=5, label='Precision-Recall curve')\n",
    "plt.title('Recall vs Precision')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### While taking recall and precision into account, there is always a tradeoff if we want to miss a fraud data or flag a normal data as a fraudulent one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
